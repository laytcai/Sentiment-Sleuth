{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from twikit import Client\n",
    "import csv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# connect to reddit api\n",
    "\n",
    "def connect_to_reddit():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id='FIDwbm9bpAJBbvrHNzFBVA',      \n",
    "        client_secret='YCCM5UGOJKn0gP2LNViTTpZPHMzQgQ',\n",
    "        user_agent='u/FecesTornado'    \n",
    "    )\n",
    "    return reddit"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ask user for topics and source to scrape from\n",
    "\n",
    "topic = input(\"what are you interested in? \")\n",
    "source = int(input(\"which source would you like to pool from: 1 for X, 2 for reddit, 3 for both.\"))\n",
    "numcomm = 9\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# strips media links and new lines from tweets\n",
    "\n",
    "def clean_link(text):\n",
    "    text = text.encode(\"charmap\", errors=\"ignore\").decode(\"charmap\")\n",
    "    text = text.replace('\\n',' ')\n",
    "    if('https' not in text): return text\n",
    "    loc = text.index('https')\n",
    "    return text[:loc] + text[loc+23:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if source==2:\n",
    "    \n",
    "    # scrape from reddit\n",
    "    \n",
    "    reddit=connect_to_reddit()\n",
    "    \n",
    "    # set up headers for csv\n",
    "    data = [[\"text\", \"likes\", \"batch\"]]\n",
    "    batch = 0\n",
    "    for submission in tqdm(reddit.subreddit('all').search(topic)):\n",
    "        # add post and comments to data, including number of upvotes\n",
    "        temp = []\n",
    "        temp.append((submission.title.replace(\"\\n\", \" \"), submission.ups, batch))\n",
    "        submission.comment_sort = \"best\"\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        comments = submission.comments\n",
    "        \n",
    "        # ensure against index out of bounds\n",
    "        for i in range(0, min(len(comments),numcomm)):\n",
    "            if isinstance(comments[i], MoreComments):\n",
    "                continue\n",
    "            temp.append((comments[i].body.replace(\"\\n\", \" \"), comments[i].ups, batch))\n",
    "        batch+=1\n",
    "        # add post data to overall data\n",
    "        data += temp\n",
    "\n",
    "elif source == 1:\n",
    "    #sign in to twitter\n",
    "    USERNAME = 'YOUR_REDDIT_USERNAME'\n",
    "    EMAIL = 'YOUR_REDDIT_EMAIL'\n",
    "    PASSWORD = 'YOUR_REDDIT_PASSWARD'\n",
    "\n",
    "    client = Client('en-US')\n",
    "\n",
    "    async def main():\n",
    "        await client.login(\n",
    "            auth_info_1=USERNAME,\n",
    "            auth_info_2=EMAIL,\n",
    "            password=PASSWORD,\n",
    "            cookies_file='cookies.json'\n",
    "        )\n",
    "\n",
    "    await main()\n",
    "\n",
    "\n",
    "    # search for top twitter tweets after searching topic\n",
    "    tweets = await client.search_tweet(topic, 'Top')\n",
    "    #print(len(tweets))\n",
    "    cursor = ''\n",
    "    num = 500\n",
    "    batch = 1\n",
    "    data = [['text', 'likes', 'batch']]\n",
    "    while(len(data) < num):\n",
    "        for tweet in tweets:\n",
    "            if(tweet.text[:5] != 'https'):\n",
    "                # save post text, likes, and batch number\n",
    "                data.append([f'{tweet.user.name}: {clean_link(tweet.text).strip()}', tweet.favorite_count, batch])\n",
    "                #print(f\"Tweeter: {tweet.user.name} \\nTweet Text: {} \\nLikes: {}\\n\")\n",
    "                \n",
    "                #Aggregate Replies\n",
    "                tries = await client.get_tweet_by_id(tweet.id)\n",
    "                replies_ = tries.replies\n",
    "\n",
    "                #print(len(replies_))\n",
    "                for rep in replies_:\n",
    "                    #Save Replies into data\n",
    "                    data.append([f'{tweet.user.name}: {clean_link(rep.text).strip()}', rep.favorite_count, batch])               \n",
    "                    #print(f\"Replier: {rep.user.name} \\nReply Text: {clean_link(rep.text).strip()} \\nLikes: {rep.favorite_count}\\n\")\n",
    "\n",
    "                batch += 1\n",
    "        #Fetches next batch of data\n",
    "        tweets = await tweets.next()\n",
    "else:\n",
    "    #SAME AS ABOVE\n",
    "    #FEWER THREADS THAN SINGULAR\n",
    "    \n",
    "    reddit=connect_to_reddit()\n",
    "    data = [[\"text\", \"likes\", \"batch\"]]\n",
    "    batch = 0\n",
    "    for submission in tqdm(reddit.subreddit('all').search(topic)):\n",
    "        temp = []\n",
    "        temp.append((submission.title.replace(\"\\n\", \" \"), submission.ups, batch))\n",
    "        submission.comment_sort = \"best\"\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        comments = submission.comments\n",
    "        for i in range(0, min(len(comments),numcomm)):\n",
    "            if isinstance(comments[i], MoreComments):\n",
    "                continue\n",
    "            temp.append((comments[i].body.replace(\"\\n\", \" \"), comments[i].ups, batch))\n",
    "        batch+=1\n",
    "        data += temp\n",
    "\n",
    "    USERNAME = 'YOUR_X_USERNAME'\n",
    "    EMAIL = 'YOUR_X_EMAIL'\n",
    "    PASSWORD = 'YOUR_X_PASSWORD'\n",
    "\n",
    "    client = Client('en-US')\n",
    "\n",
    "    async def main():\n",
    "        await client.login(\n",
    "            auth_info_1=USERNAME,\n",
    "            auth_info_2=EMAIL,\n",
    "            password=PASSWORD,\n",
    "            cookies_file='cookies.json'\n",
    "        )\n",
    "\n",
    "    await main()\n",
    "\n",
    "    tweets = await client.search_tweet(topic, 'Top')\n",
    "    #print(len(tweets))\n",
    "    cursor = ''\n",
    "    num = 250\n",
    "    batch = 1\n",
    "    while(len(data) < 2*num):\n",
    "        for tweet in tweets:\n",
    "            if(tweet.text[:5] != 'https'):\n",
    "                data.append([f'{tweet.user.name}: {clean_link(tweet.text).strip()}', tweet.favorite_count, batch])\n",
    "                #print(f\"Tweeter: {tweet.user.name} \\nTweet Text: {} \\nLikes: {}\\n\")\n",
    "\n",
    "                tries = await client.get_tweet_by_id(tweet.id)\n",
    "                replies_ = tries.replies\n",
    "\n",
    "                #print(len(replies_))\n",
    "                for rep in replies_:\n",
    "                    data.append([f'{tweet.user.name}: {clean_link(rep.text).strip()}', rep.favorite_count, batch])               \n",
    "                    #print(f\"Replier: {rep.user.name} \\nReply Text: {clean_link(rep.text).strip()} \\nLikes: {rep.favorite_count}\\n\")\n",
    "\n",
    "                batch += 1\n",
    "\n",
    "        tweets = await tweets.next()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Reak csv\n",
    "with open(\"output.csv\", 'w', newline = '', encoding=\"UTF-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Input the opinion you want to compare.\n",
    "assessment_topic = input(\"What do you want to know?\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Input your API Key here.\n",
    "apikey = \"YOUR_API_KEY\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Input base url for api access\n",
    "client = OpenAI(\n",
    "    base_url='YOUR_BASE_URL',\n",
    "    api_key=apikey\n",
    ")\n",
    "df = pd.read_csv(\"output.csv\", encoding=\"utf-8\")\n",
    "user_comment = df[\"text\"].tolist()\n",
    "user_like = df[\"likes\"].tolist()\n",
    "user_batch = df[\"batch\"].tolist()\n",
    "batched_comments = []\n",
    "batched_likes = []\n",
    "bid = -1\n",
    "for i in range(len(user_batch)):\n",
    "    if i == 0 or user_batch[i] != user_batch[i-1]:\n",
    "        bid += 1\n",
    "        batched_comments.append([])\n",
    "        batched_likes.append([])\n",
    "    batched_comments[bid].append(user_comment[i])\n",
    "    batched_likes[bid].append(user_like[i])\n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": f\"Analyze how much each user agrees with {assessment_topic}, from -100(strongly disagree) \"\n",
    "                                                    f\"to 100(strongly agree).Response only numbers. \"\n",
    "                                          f\"Each string in the list is a user comment, and each list is a whole thread. \"\n",
    "                                          f\"Must generate scores for all users. Separate numbers with ; \"\n",
    "                                          f\"Score 0 if neutral, Score -101 if irrelevant.\"}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Pass in all user comments, and use LLM to assess.\n",
    "scores =[]\n",
    "m_likes = []\n",
    "for i in tqdm(range(len(batched_comments))):\n",
    "    line = batched_comments[i]\n",
    "    messages = [\n",
    "        system_message,\n",
    "        {\"role\": \"user\", \"content\": f\"{line} ;; Response only numbers, Score 0 if neutral, Score -101 if irrelevant, separate numbers with; make sure you have all {len(line)} user ratings.\"}\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        extra_headers={},\n",
    "        extra_body={},\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    batchdata = completion.choices[0].message.content.strip().replace(\"\\n\",\";\").replace(\".\",\"\").replace(\" \", \"\").split(\";\")\n",
    "    if len(batchdata) < len(batched_likes[i]):\n",
    "        print(f\"batch num {i} does not match\")\n",
    "        batched_likes[i] = batched_likes[i][:len(batchdata)]\n",
    "    scores.extend(batchdata)  # Strip extra spaces/newlines\n",
    "    m_likes.extend(batched_likes[i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Preview scores\n",
    "print(scores)\n",
    "print(type(scores))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Summarize all data\n",
    "sint = []\n",
    "i_likes = []\n",
    "for i in m_likes:\n",
    "    try:\n",
    "        i_likes.append(int(i))\n",
    "    except ValueError:\n",
    "        i_likes.append(0)\n",
    "for x in scores:\n",
    "    try:\n",
    "        sint.append(int(x))\n",
    "    except ValueError:\n",
    "        sint.append(-101) \n",
    "sint = np.array(sint)\n",
    "count_neg_10_to_neg_100 = np.sum((sint >= -100) & (sint < -10))\n",
    "count_pos_10_to_100 = np.sum((sint > 10) & (sint <= 100))\n",
    "count_neg_10_to_10 = np.sum((sint >= -10) & (sint <= 10))\n",
    "count_irr = np.sum(sint==-101)\n",
    "# \n",
    "# # Add up all scores and divide by the total number of scores (calculate the mean)\n",
    "average_score = np.sum(sint[sint != -101])/np.sum(sint != -101)\n",
    "# Sum all values that are not equal to -101\n",
    "# # Return the results\n",
    "print(f\"Disagree: {count_neg_10_to_neg_100} | Agree: {count_pos_10_to_100} | Natural: {count_neg_10_to_10} | Irrelevant: {count_irr} | Average: {average_score}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Gerenate summary of viewpoints from LLM.\n",
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : f\"You want to summarize in one paragraph the prompt {assessment_topic} base on the number we obtained. Disregard what numbers mean, only tell us the summary.\"},\n",
    "    {\"role\" : \"user\", \"content\" : f\"This is the data {sint}, where -101 means irrelevant, -100 to -10 is disagree, -10 to 10 is neutral, 10 to 100 is agree. {i_likes} is the like that each score has, you can correspond likes to scores for your summary.\"}\n",
    "    ]\n",
    "completion = client.chat.completions.create(\n",
    "    extra_headers={},\n",
    "    extra_body={},\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=messages\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
